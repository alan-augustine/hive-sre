## Hive Upgrade Check (u3)

Review Hive Metastore Databases and Tables for upgrade or simply to evaluate potential issues.  The intent was to make that process much more prescriptive and consumable by Cloudera customers.  The application is 'Apache Hive' based, so it should work against both 'HDP', 'CDH', and 'CDP' clusters.  See additional details about [upgrading HDP2 and CDP 5/6 to CDP](./tocdp.md).

[Download, Requirements, and Configuration Details](./README.md)

### Quick Start (What to do!!!)

This process can be a long running process.  I recommend using `screen` or `tmux` sessions to run it.  That way the session won't get terminated when you disconnect from the host and you can easily reattach later to see the progress.

The process should start registering counts pretty quickly for each of the sub-checks.  If you do not see these counts moving, or they show `0`, check your connection to the Metastore database (IE: URL, RDBMS Driver(needs to be in $HOME/.hive-sre/aux_libs), Username, password, and/or permissions).

Logs for the application are at $HOME/.hive-sre/logs.  Check those for details on progress and/or problems.

#### Use the Platform flag to run ONLY the important reports.

Use one of `-cdh`, `-hdp2`, or `-hdp3` depending on your source platform to run only the reports that make sense.  If you don't use one of these options, that's fine.  Just know that you'll be running MORE reports than you need and the process will run much longer.  Just saying....

EACH report contains a description **within** the report on what actions should be taken.

If you are running the default upgrade and NOT doing the CDP Expedited Hive Upgrade Process for [CDH5](https://docs.cloudera.com/cdp-private-cloud-upgrade/latest/upgrade-cdh/topics/hive-expedite-cdh-overview.html), [CDH6](https://docs.cloudera.com/cdp-private-cloud-upgrade/latest/upgrade-cdh6/topics/hive-expedite-cdh-overview.html), [HDP2](https://docs.cloudera.com/cdp-private-cloud-upgrade/latest/upgrade-hdp/topics/hive-expedite-hdp-overview.html) there is an HSMM (Hive Strict Managed Migration) that runs AFTER the binaries and components have been upgraded to CDP's Hive 3.  This process (HSMM) will fail if the issues/errors reported in reports [1 - Table / Partition Location Scan - Missing Directories](./sample_reports/u3/loc_scan_missing_dirs.md),  [5 - Questionable Serde's Check](./sample_reports/u3/hms_checks.md) are not fixed BEFORE doing the upgrade.

Metadata Updates, like legacy-managed table conversions are done after the components and configurations have been updated to CDP.  Therefore, the report actions for [3 - Hive 3 Upgrade Checks - Managed Non-ACID to ACID Table Migrations](./sample_reports/u3/managed_upgrade_2_acid.sql) and [6 - Provide Script for ACID tables missing the 'bucketing_version' property](./sample_reports/u3/acid_bucketing_update.sql)  will be done by HSMM.

If you choose to do the Expedited Hive Upgrade Process detailed/linked above, you need to run the actions in reports 3 and 6 BEFORE you release the 'tables' (not hive) to the consumer.  Which means these can be done post upgrade.  Failure to do these, isn't the end of the world.  The tables will throw errors util the scripts to upgrade their metadata is complete.

##### HDP2

Runs reports:
- [1](./sample_reports/u3/loc_scan_missing_dirs.md) - Table / Partition Location Scan - Missing Directories 
- [3](./sample_reports/u3/managed_upgrade_2_acid.sql) - Hive 3 Upgrade Checks - Managed Non-ACID to ACID Table Migrations 
- [4](./sample_reports/u3/managed_compactions.sql) - Hive 3 Upgrade Checks - Compaction Check 
- [5](./sample_reports/u3/hms_checks.md) - Hive Metastore Check <br/> - Questionable Serde's Check<br/>  - Questionable Serde's Check<br/> - Leagcy Kudu Serde Report<br/> - Legacy Decimal Scale/Precision Check<br/> - List Databases with Table/Partition Counts 
- [6](./sample_reports/u3/acid_bucketing_update.sql) - Provide Script for ACID tables missing the 'bucketing_version' property 

##### HDP3

Runs reports:
- [1](./sample_reports/u3/loc_scan_missing_dirs.md) - Table / Partition Location Scan - Missing Directories
- [5](./sample_reports/u3/hms_checks.md) - Hive Metastore Check <br/> - Questionable Serde's Check<br/>  - Questionable Serde's Check<br/> - Leagcy Kudu Serde Report<br/> - Legacy Decimal Scale/Precision Check<br/> - List Databases with Table/Partition Counts
- [6](./sample_reports/u3/acid_bucketing_update.sql) - Provide Script for ACID tables missing the 'bucketing_version' property

##### CDH

Runs reports:

- [1](./sample_reports/u3/loc_scan_missing_dirs.md) - Table / Partition Location Scan - Missing Directories
- [3](./sample_reports/u3/managed_upgrade_2_acid.sql) - Hive 3 Upgrade Checks - Managed Non-ACID to ACID Table Migrations
- [5](./sample_reports/u3/hms_checks.md) - Hive Metastore Check <br/> - Questionable Serde's Check<br/>  - Questionable Serde's Check<br/> - Leagcy Kudu Serde Report<br/> - Legacy Decimal Scale/Precision Check<br/> - List Databases with Table/Partition Counts
- [6](./sample_reports/u3/acid_bucketing_update.sql) - Provide Script for ACID tables missing the 'bucketing_version' property

### Testing

I have tested this against MariaDB 10.2, Postgres, and Oracle.  I have seen reports of SQL issues against MySql 5.6, so this process will certainly have issues there.  If you run this in other DB configs, I would like to hear from you about it.

### Known Issues

For a while during the evolution of Hive 3, there was a separate 'catalog' for Spark.  The queries in this process do NOT consider this alternate catalog and may yield cross products in some areas if the 'spark' catalog was used at any point.  Even though this tool was designed for Hive 1 and 2 in mind, it can be run against Hive 3.  We do NOT include criteria regarding the 'catalog'.

### Assumptions

- Ran from a node on the cluster
    - That includes clients and configuration files for HDFS
- Ran by a user that has READ privileges to all HDFS directories.
- If cluster is 'kerberized', the user has a valid Kerberos Ticket 'before' starting the application.
- Drivers for the HMS database are available.
- The configuration file has been defined
- The HMS Metastore DB is on a supported RDBMS for the platform (version matters!)

### Application Help

```
Launching: u3
v.2.4.0.10-SNAPSHOT
usage: Sre
 -cdh,--cloudera-data-hub               Run processes that make sense for
                                        CDH.
 -cfg,--config <arg>                    Config with details for the Sre
                                        Job.  Must match the either sre or
                                        u3 selection. Default:
                                        $HOME/.hive-sre/cfg/default.yaml
 -db,--database <arg>                   Comma separated list of Databases.
                                        Will override config. (upto 100)
 -dbRegEx,--database-regex <arg>        A RegEx to match databases to
                                        process
 -edb,--exclude-database <arg>          Comma separated list of Databases
                                        to 'exclude' from run.  Will
                                        override config. (upto 100)
 -h,--help                              Help
 -hdp2,--hortonworks-data-platfrom-v2   Run processes that make sense for
                                        HDP2.
 -hdp3,--hortonworks-data-platfrom-v3   Run processes that make sense for
                                        HDP3.
 -hfw,--hive-framework <arg>            The custom HiveFramework check
                                        configuration. Needs to be in the
                                        'Classpath'.
 -i,--include <arg>                     Comma separated list of process
                                        id's to run.  When not specified,
                                        ALL processes are run.
 -o,--output-dir <arg>                  Output Directory to save results
                                        from Sre.
 -scc,--skip-command-checks             Don't process the command checks
                                        for the process.
 ```

To limit which process runs, use the `-i` (include) option at the command line with a comma separated list of ids (below) of desired processes.

| id (link to sample report) | process |
|:---|:---|
| [1](./sample_reports/u3/loc_scan_missing_dirs.md) | Table / Partition Location Scan - Missing Directories |
| [2](./sample_reports/u3/bad_filenames_for_orc_conversion.md) | Table / Partition Location Scan - Bad ACID ORC Filenames |
| [3](./sample_reports/u3/managed_upgrade_2_acid.sql) | Hive 3 Upgrade Checks - Managed Non-ACID to ACID Table Migrations |
| [4](./sample_reports/u3/managed_compactions.sql) | Hive 3 Upgrade Checks - Compaction Check |
| [5](./sample_reports/u3/hms_checks.md) | Hive Metastore Check <br/> - Questionable Serde's Check<br/>  - Questionable Serde's Check<br/> - Leagcy Kudu Serde Report<br/> - Legacy Decimal Scale/Precision Check<br/> - List Databases with Table/Partition Counts |
| [6](./sample_reports/u3/acid_bucketing_update.sql) | Provide Script for ACID tables missing the 'bucketing_version' property |

### Running

Run the **Hive Metastore Checks** Report first (`-i 5`) to get a list of databases with table and partition counts.  With this information, you can develop a strategy to run the tool in parts using the `-db` option.  Multi-tasking can be controlled in the configuration files `parallelism` setting.

To ease the launch of the application below, configure these core environment variables.

### Output

The output is a set of files with actions and error (when encountered).  The files maybe `txt` files or `markdown`.  You may want to use a `markdown` viewer for easier viewing of those reports.  The markdown viewer needs to support [github markdown tables](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet#tables) .

#### Examples
*All Hive Databases* - Will run ALL processes.
```
hive-sre u3 -cfg /tmp/test.yaml -o ./sre-out
```

*Targeted Hive Database* - Will run ALL processes on the 'priv_dstreev' hive database.
```
hive-sre u3 -db priv_dstreev -cfg /tmp/test.yaml -o ./sre-out
```

*Run ONLY compaction check on All Hive Database* - Using the `-i` option to run only the 'compaction check' sub routine.
```
hive-sre u3 -cfg /tmp/test.yaml -o ./sre-out -i 4
```

### Check and Validations Performed

NO action is taken by this process.  The output of each section will contain 'actions' for you to take when a scenario materializes.  It is up to you to carry out those actions after reviewing them.

1. Hive 3 Upgrade Checks - Locations Scan
    - Missing Directories
    > Missing Directories cause the upgrade conversion process to fail.  To prevent that failure, there are two choices for a 'missing directory'.  Either create it of drop the table/partition.

    - You have two choices based on the output of this process.                        
        - RECOMMENDED: Drop the table/partition OR
        - Create the missing directory referenced by the table/partition (empty directories have been known to cause hive table CBO issues, leading to performance slowdowns).
    - The output file from this process will provide commands to accomplish which ever direction you choose. Use 'hive' to run the sql statements.  Use [hadoopcli](https://github.com/dstreev/hadoop-cli) to run the 'hdfs' commands in bulk.
2. Hive 3 Upgrade Checks - Bad ORC Filenames
    - Bad Filename Format
    > Tables that would be convert from a Managed Non-Acid table to an ACID transactional table require the files to match a certain pattern. This process will scan the potential directories of these tables for bad filename patterns.  When located, it will indicate which tables/partitions have been file naming conventions that would prevent a successful conversion to ACID.  The best and easiest way to correct these files names is to use HiveSQL to rewrite the contents of the table/partition with a simple 'INSERT OVERWRITE TABLE xxx SELECT * FROM xxx'.  This type of statement will replace the current bad filenames with valid file names by rewriting the contents in HiveSQL.
3. Hive 3 Upgrade Checks - Managed Table Migrations
    - Ownership Check
    - Conversion to ACID tables
    > This process will list tables that will and 'could' be migrated to "Managed ACID" tables during the upgrade process.  If these tables are used by Spark OR data is managed by a separate process that interacts with the FileSystem, DO NOT LET THESE conversion happen.  The output of this process will supply Hive DDL commands to convert these tables to "EXTERNAL / PURGE" tables in Hive 3, which is the same as the 'classic' Hive 1/2 Managed Non-Acid table.                                                                      
4. Hive 3 Upgrade Checks - Compaction Check
    - Compaction Check
    > Review ACID tables for 'delta' directories.  Where 'delta' directories are found, we'll 
5. Metastore Report
   - Questionable Serde's Check
   > Will list tables using SERDE's that are not standard to the platform.
      - Action here either:
        - Remove the table using the SERDE, if the SERDE isn't available
        - Ensure the SERDE is available during the upgrade so table can be evaluated.
   - Legacy Kudu Serde Report
     > Early versions of Hive/Impala tables using Kudu were built before Kudu became an Apache Project.  Once it became an Apache Project, the base Kudu Storage Handler classname changed.  This report locates and reports on tables using the legacy storage handler class.
   - Legacy Decimal Scale and Precision Check
     > When the `DECIMAL` data type was first introduced in Hive 1, it did NOT include a Scale or Precision element.  This causes issues in later integration with Hive and Spark.  We'll identify and suggest corrective action for tables where this condition exists.
   - Managed Table Shadows
   > In Hive 3, Managed tables are 'ACID' tables.  Sharing a location between two 'ACID' tables will cause compaction issues and data issues.  These need to be resolved before the upgrade.
   - Database / Table and Partition Counts
    > Use this to understand the scope of what is in the metastore.

### Getting Ready for a Hive 1/2 to Hive 3 Upgrade

The reports created by this application will provide you with a prescriptive set of actions that will prepare and accelerate the Hive 1/2 to Hive 3 upgrade.

Hive consists of two parts:  Metadata and Data.  Since these are not strictly linked, they do get out of sync.  The process 'Location Scans' above finds tables and partitions that don't have a matching file system location.  Use the details of this report to 'clean up' the metastore and filesystem.

Use the reports and scripts provided to build a plan for your upgrade.  The latest versions of `u3` will create a file called `hsmm_includelist.yaml`.  This contains a list of Databases and Tables that need attention.  Use the contents of this list to choose the actions you'll take for the upgrade.  The choices are:

- Run the HiveStrictManagedMigration process against these targeted database and tables to avoid its default iteration over ALL DB's and tables.
- Run the scripts suggested by the reports to migrate your tables.  NOTE: Scripts aren’t included to address legacy Kudu storage handler classes (CDH Only).  Run the HSMM process on those DB’s and Tables identified in the report.

Note that the HiveStrictManagedMigration process DOES make adjustments to database definitions as a part of the upgrade.  The CDP upgrade docs will include examples on how to ensure that process is run against the DB's and not any tables, to minimize the time in that process. 